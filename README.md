# ğŸ¤– Assistente Inteligente MultitemÃ¡tico

Uma soluÃ§Ã£o de assistente conversacional que integra Processamento de Linguagem Natural (NLP), Retrieval-Augmented Generation (RAG) e Grandes Modelos de Linguagem (LLMs). A plataforma se destaca por seu suporte a mÃºltiplos domÃ­nios (temas de agente) e mÃºltiplos temas visuais para o frontend. O sistema carrega dinamicamente a base de conhecimento, intenÃ§Ãµes e a personalidade do agente, permitindo que o usuÃ¡rio personalize tanto a especialidade do assistente quanto a aparÃªncia da interface de chat.

## ğŸš€ Funcionalidades

- **Interface Web Interativa:** Frontend completo construÃ­do com HTML, CSS e JavaScript puro.
- **SeleÃ§Ã£o DinÃ¢mica de Agente:** Permite ao usuÃ¡rio escolher com qual "personalidade" de agente deseja interagir (ex: Especialista em IA, Consultor Financeiro, CrÃ­tico LiterÃ¡rio, etc.).
- **SeleÃ§Ã£o de Tema Visual:** Personalize a aparÃªncia do chat com temas como PadrÃ£o, Matrix, Cyberpunk e Exterminador do Futuro.
- **Arquitetura MultitemÃ¡tica:** Configure mÃºltiplos assistentes, cada um com sua prÃ³pria base de conhecimento (`knowledge`), conjunto de intenÃ§Ãµes (`intents.json`) e persona (`prompt.txt`).
- **Pipeline de NLP:**
    - CompreensÃ£o de linguagem natural com detecÃ§Ã£o de intenÃ§Ãµes (especÃ­fica por tema).
    - AnÃ¡lise de sentimentos e emoÃ§Ãµes.
    - ExtraÃ§Ã£o de Entidades Nomeadas (NER).
- **Busca SemÃ¢ntica (RAG):** Utiliza FAISS e SentenceTransformers para buscar informaÃ§Ãµes relevantes na base de conhecimento do tema selecionado.
- **GeraÃ§Ã£o de Respostas com LLM:** Integra-se com a API da OpenAI para gerar respostas contextuais e personalizadas.
- **MemÃ³ria Conversacional:** MantÃ©m o histÃ³rico da conversa por sessÃ£o.
- **Escalonamento para Atendimento Humano:** LÃ³gica de decisÃ£o para escalonamento, com integraÃ§Ã£o via webhook para plataformas como N8N.
- **ContainerizaÃ§Ã£o:** Suporte completo para execuÃ§Ã£o com Docker e Docker Compose.

## ğŸ§± Arquitetura

- **Backend:** FastAPI (Python)
- **Frontend:** HTML, CSS, JavaScript (sem frameworks)
- **LLM:** OpenAI (gpt-3.5-turbo, gpt-4, etc.)
- **Busca SemÃ¢ntica (RAG):** FAISS + SentenceTransformers
- **NLP:** spaCy, Transformers, SentenceTransformers
- **OrquestraÃ§Ã£o/Workflow:** N8N (via Webhook)
- **ServiÃ§os Adicionais (via Docker):** Redis, PostgreSQL

## ğŸ“ Estrutura do Projeto

A estrutura foi organizada para separar claramente o backend, o frontend, os temas dos agentes e a configuraÃ§Ã£o de containerizaÃ§Ã£o.

```txt
nlp/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ nlp/
â”‚   â”œâ”€â”€ rag/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ css/
â”‚       â”œâ”€â”€ default.css
â”‚       â”œâ”€â”€ matrix.css
â”‚       â”œâ”€â”€ cyberpunk.css
â”‚       â””â”€â”€ terminator.css
â”œâ”€â”€ n8n-fluxos/
â”‚   â””â”€â”€ ChatBot.json
â”œâ”€â”€ themes/
â”‚   â”œâ”€â”€ ai_specialist/
â”‚   â”œâ”€â”€ banking/
â”‚   â”œâ”€â”€ fuzzy_logic_expert/
â”‚   â””â”€â”€ literature/
â”‚       â”œâ”€â”€ knowledge/
â”‚       â”‚   â””â”€â”€ *.txt, *.pdf
â”‚       â”œâ”€â”€ intents.json
â”‚       â””â”€â”€ prompt.txt
â”œâ”€â”€ build_all_themes.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## âš™ï¸ Como Funciona

1.  **Frontend:** O usuÃ¡rio seleciona um tema de agente (ex: "Banking") e um tema visual (ex: "Cyberpunk") e envia uma mensagem.
2.  **Backend (FastAPI):** A `main.py` recebe a requisiÃ§Ã£o.
3.  **Carregamento DinÃ¢mico:** O sistema carrega os componentes do agente selecionado (buscador RAG, detector de intenÃ§Ã£o, prompt do sistema) com base no tema escolhido.
4.  **Pipeline NLP:** A mensagem do usuÃ¡rio Ã© processada para extrair a intenÃ§Ã£o, entidades, sentimento e emoÃ§Ãµes.
5.  **DecisÃ£o de Escalonamento:** O `agent/decision.py` avalia se a conversa deve ser escalada para um atendente humano.
6.  **Busca RAG:** Se nÃ£o for escalonado, o `rag/search.py` busca na base de conhecimento vetorial (Ã­ndice FAISS) por informaÃ§Ãµes relevantes para a consulta.
7.  **GeraÃ§Ã£o com LLM:** O prompt do sistema, o histÃ³rico da conversa, a pergunta do usuÃ¡rio e o contexto recuperado pelo RAG sÃ£o enviados para a API da OpenAI.
8.  **Resposta:** A resposta gerada pelo LLM Ã© enviada de volta para o frontend e exibida ao usuÃ¡rio.

## ğŸ“¦ InstalaÃ§Ã£o e ExecuÃ§Ã£o

Existem duas maneiras de executar o projeto: localmente com um ambiente Python ou usando Docker.

### OpÃ§Ã£o 1: ExecuÃ§Ã£o Local

**1. Clone o repositÃ³rio e instale as dependÃªncias:**

```bash
# git clone https://github.com/seu-usuario/assistente-bancario.git
cd assistente-bancario
python -m venv venv
# No Windows
venv\Scripts\activate
# No Linux/macOS
# source venv/bin/activate
pip install -r requirements.txt
```

**2. Baixe o modelo de linguagem para spaCy:**

```bash
python -m spacy download pt_core_news_sm
```

**3. Configure sua chave da API da OpenAI:**

Crie um arquivo chamado `.env` na raiz do projeto e adicione sua chave:

```
OPENAI_API_KEY="sk-..."
```

**4. Construa os Ãndices de Conhecimento:**

Antes de rodar a aplicaÃ§Ã£o, gere os Ã­ndices de busca para cada tema de agente. Este script irÃ¡ processar os arquivos em `themes/*/knowledge/` e criar um Ã­ndice FAISS (`faiss_index`) e um arquivo de passagens (`passages.pkl`) em cada diretÃ³rio de tema.

```bash
python build_all_themes.py
```

**5. Inicie o Servidor Backend:**

Execute o servidor a partir da raiz do projeto. O Uvicorn irÃ¡ servir tanto a API quanto o frontend.

```bash
cd backend

uvicorn main:app --reload
```

**6. Acesse a AplicaÃ§Ã£o:**

Abra seu navegador e acesse `http://127.0.0.1:8000`.

### OpÃ§Ã£o 2: ExecuÃ§Ã£o com Docker (Recomendado)

O `docker-compose` orquestra todos os serviÃ§os necessÃ¡rios, incluindo o backend, Redis e Postgres.

**1. PrÃ©-requisitos:**
   - Docker instalado e em execuÃ§Ã£o.
   - Docker Compose instalado.

**2. Configure a Chave da API:**
   - Crie o arquivo `.env` na raiz do projeto, como descrito na execuÃ§Ã£o local.

**3. Construa e Inicie os Containers:**

Execute o seguinte comando na raiz do projeto:

```bash
docker-compose up --build
```

Este comando irÃ¡:
- Construir a imagem do Python com todas as dependÃªncias.
- Baixar o modelo do spaCy.
- Iniciar os serviÃ§os de Redis e Postgres.
- Iniciar a aplicaÃ§Ã£o Python, que por sua vez irÃ¡ construir os Ã­ndices dos temas e iniciar o servidor.

**4. Acesse a AplicaÃ§Ã£o:**

Abra seu navegador e acesse `http://localhost:8000`.

## ğŸ¨ Temas (Agentes)

Para adicionar um novo agente, basta criar uma nova pasta dentro do diretÃ³rio `themes/`. Por exemplo, para um agente "historiador":

1.  Crie a pasta `themes/historian/`.
2.  Adicione uma pasta `knowledge/` dentro de `historian/` com seus arquivos `.txt` ou `.pdf`.
3.  Crie um arquivo `intents.json` com as intenÃ§Ãµes e exemplos especÃ­ficos para histÃ³ria.
4.  Crie um arquivo `prompt.txt` definindo a persona e as regras do agente historiador.

Depois de adicionar o novo tema, execute novamente o script `build_all_themes.py` (se estiver rodando localmente) ou reinicie os containers do Docker para que o novo Ã­ndice seja criado. O novo agente aparecerÃ¡ automaticamente no seletor do frontend.
